{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10638055,"sourceType":"datasetVersion","datasetId":6586604}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom glob import glob\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Activation\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport imageio","metadata":{"id":"K1NERW_nAhTU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 40\ntf.random.set_seed(seed=40)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_HEIGHT = 512\nIMG_WIDTH = 512\nBATCH_SIZE = 16\n\nval_split = 0.2\nn_classes = 2  # change this to 4 when you run 4 classes\nclass_names = [\"soil\", \"residue\"] # specify the classes when you run for 4 classes\n#class_names = [\"residue_sunlit\", \"residue_shaded\", \"background_sunlit\", \"background_shaded\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def compute_confusion_matrix(y_true, y_pred, num_classes=n_classes):\n    \"\"\"Compute confusion matrix for multi-class segmentation\"\"\"\n    # Flatten the arrays (each pixel is treated as a classification label)\n    y_true_flat = y_true.flatten()\n    y_pred_flat = y_pred.flatten()\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n\n    return cm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(cm):\n    tp = np.diag(cm)  # True positives\n    fp = np.sum(cm, axis=0) - tp  # False positives\n    fn = np.sum(cm, axis=1) - tp  # False negatives\n\n    precision = tp / (tp + fp + 1e-6)  # Avoid division by zero\n    recall = tp / (tp + fn + 1e-6)\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n\n    return precision, recall, f1_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(cm):\n    \"\"\"Compute IoU for each class\"\"\"\n    tp = np.diag(cm)\n    fp = np.sum(cm, axis=0) - tp\n    fn = np.sum(cm, axis=1) - tp\n    iou = tp / (tp + fp + fn + 1e-6)  # Avoid division by zero\n\n    mean_iou = np.mean(iou)  # Mean IoU across all classes\n    return iou, mean_iou","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing dataset","metadata":{"id":"KbM6QZ7n-Z8k"}},{"cell_type":"code","source":"def load_and_preprocess_image(image_path):\n    \"\"\"Loads and preprocesses an image from the given path.\"\"\"\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n    return tf.cast(image, tf.int64)  # (IMG_HEIGHT, IMG_WIDTH, 3)\n    \ndef load_and_preprocess_mask(mask_path):\n    \"\"\"Loads and preprocesses a mask from the given path.\"\"\"\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=1)\n    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    mask = tf.one_hot(tf.cast(mask, tf.int32), depth=n_classes)\n    return tf.squeeze(mask, -2) # (IMG_HEIGHT, IMG_WIDTH, n_classes)\n\ndef extract_image_name(image_path):\n    \"\"\"Extracts the filename without extension from the image path.\"\"\"\n    filename = tf.strings.split(image_path, os.sep)[-1]  # Extract filename\n    return tf.strings.split(filename, '.')[0]  # Remove extension\n    \ndef parse_image_and_mask(image_path, mask_path):\n    \"\"\"Parses and processes an image and its corresponding mask.\"\"\"\n    image = load_and_preprocess_image(image_path)\n    mask = load_and_preprocess_mask(mask_path)\n    image_name = extract_image_name(image_path)\n    return image, mask, image_name\n\ndef create_dataset(image_paths, mask_paths):\n    \"\"\"Creates a TensorFlow dataset from image and mask paths.\"\"\"\n    path_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n    return path_dataset.map(parse_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Specify the base directory\nroot = \"/kaggle/input/crop-residue-new-jpg/Training/\"\n\n# Specify the paths for input images and masks\nimage_paths = sorted(glob(os.path.join(root, \"input/*\")))\nmask_paths = sorted(glob(os.path.join(root, \"output/converted/jpg_files/*\")))\n\n# Create the dataset\ndataset = create_dataset(image_paths, mask_paths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_predicted_image(image_name, pred):\n    \"\"\"\n    Save Predicted Images\n\n    Args:\n        image_name (tf.Tensor): Tensor containing the image filename.\n        pred (numpy array, optional): The predicted mask (one-hot encoded). Defaults to None.\n    \"\"\"\n    image_name_str = image_name.numpy().decode('utf-8')\n    # Convert predictions to class labels\n    pred_mask = np.argmax(pred, axis=-1) + 1  # Shift classes from [0, n_classes-1] to [1, n_classes]\n    \n    # Save predicted mask as an image\n    pred_img_path = f\"/kaggle/working/{image_name_str}.png\"\n    imageio.imwrite(pred_img_path, pred_mask.astype(np.uint8))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_example(image, mask, image_name, pred=None):\n    \"\"\"\n    Plots the input image, its corresponding masks, and optionally the predicted masks.\n\n    Args:\n        image (numpy array or tensor): The input image.\n        mask (numpy array or tensor): The ground truth segmentation mask (one-hot encoded).\n        image_name (tf.Tensor): Tensor containing the image filename.\n        pred (numpy array, optional): The predicted mask (one-hot encoded). Defaults to None.\n    \"\"\"\n\n    image_name_str = image_name.numpy().decode('utf-8')\n\n    nrows = 2 if pred is None else 3\n    ncols = n_classes\n    \n    plt.figure(figsize=(20, 10))\n    \n    # Display input image\n    plt.subplot(nrows, ncols, 1)\n    plt.imshow(image)\n    plt.title(f\"Image: {image_name_str}\")\n    plt.axis(\"off\")\n\n    # Display ground truth masks\n    for i in range(n_classes):\n        plt.subplot(nrows, ncols, i + n_classes + 1)\n        plt.imshow(mask[:, :, i])\n        plt.title(f\"Mask: {class_names[i]}\")\n        plt.axis(\"off\")\n        \n    if pred is not None:\n        # Display predicted masks\n        for i in range(n_classes):\n            plt.subplot(nrows, ncols, i + n_classes * 2 + 1)\n            plt.imshow(pred[:, :, i])\n            plt.title(f\"Prediction: {class_names[i]}\")\n            plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, mask, image_name = next(iter(dataset))\nplot_example(image, mask, image_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determine dataset size and validation split\ndataset_size = len(dataset)\nval_size = int(val_split * dataset_size)\n\nprint(f\"Training samples: {dataset_size - val_size}\")\nprint(f\"Validation samples: {val_size}\")\n\n# Shuffle the dataset\nshuffled_dataset = dataset.shuffle(buffer_size=1000, seed=seed)\n\n# Split into training and validation sets\ntrain_ds = shuffled_dataset.skip(val_size)\nval_ds = shuffled_dataset.take(val_size)\n\n# Apply dataset optimizations\ntrain_ds = (train_ds\n            .shuffle(buffer_size=1000, seed=seed)\n            .cache()\n            .batch(BATCH_SIZE)\n            .prefetch(buffer_size=tf.data.AUTOTUNE))\n\nval_ds = (val_ds\n          .batch(BATCH_SIZE)\n          .cache()\n          .prefetch(buffer_size=tf.data.AUTOTUNE))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set class weights","metadata":{}},{"cell_type":"code","source":"# Filter out masks from training data\ntrain_masks = tfds.as_numpy(train_ds.map(lambda img, mask, image_name: mask))\n\n# Compute class weights for each batch\nclass_weights_list = []\n\nfor i, batch in enumerate(train_masks):\n    print(f\"Processing batch {i + 1}/{len(train_masks)}\")\n\n    batch = tf.math.argmax(batch, axis=-1)\n    flattened_labels = tf.cast(tf.reshape(batch, -1), tf.int32).numpy()  # Flatten to 1D array\n\n    # Compute class weights (balanced based on pixel frequency)\n    class_weights = compute_class_weight(class_weight=\"balanced\", classes=list(range(n_classes)), y=flattened_labels)\n    class_weights_list.append(class_weights)\n\n# Average class weights across all batches\nclass_weights = np.mean(class_weights_list, axis=0)\n\n# Print final class weights\nprint(f\"Class weights: {class_weights}\")\nprint({class_names[i]: class_weights[i] for i in range(n_classes)})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# U-Net Model","metadata":{"id":"YxpdOHsFyOtE"}},{"cell_type":"code","source":"def conv_block(x, filters, dropout_rate, l2_reg):\n    \"\"\"Convolutional block: Conv -> BatchNorm -> ReLU -> Dropout -> Conv -> BatchNorm -> ReLU\"\"\"\n\n    x = Conv2D(filters, (3, 3), padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    if dropout_rate > 0:\n        x = Dropout(dropout_rate)(x)\n    x = Conv2D(filters, (3, 3), padding='same', use_bias=False,\n           kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef encoder_block(x, filters, dropout_rate, l2_reg):\n    \"\"\"Encoder block with convolution and max pooling\"\"\"\n    x = conv_block(x, filters, dropout_rate, l2_reg)\n    p = MaxPooling2D((2, 2))(x)\n    return x, p\n    \ndef decoder_block(x, skip_connection, filters, dropout_rate, l2_reg):\n    \"\"\"Decoder block with upsampling and skip connection\"\"\"\n    x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same',\n                        kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(x)\n    x = concatenate([x, skip_connection])\n    x = conv_block(x, filters, dropout_rate, l2_reg)\n    return x\n\ndef multi_unet_model(n_classes, img_height, img_width, img_channels):\n    inputs = Input((img_height, img_width, img_channels))\n    s = inputs\n    \n    # Contraction path, encoder\n    c1, p1 = encoder_block(inputs, filters=16, dropout_rate=0, l2_reg=0.0001)\n    c2, p2 = encoder_block(p1, filters=32, dropout_rate=0, l2_reg=0.0001)\n    c3, p3 = encoder_block(p2, filters=64, dropout_rate=0, l2_reg=0.0001)\n    c4, p4 = encoder_block(p3, filters=128, dropout_rate=0, l2_reg=0.001)\n    c5, p5 = encoder_block(p4, filters=256, dropout_rate=0, l2_reg=0.001)\n    c6, p6 = encoder_block(p5, filters=512, dropout_rate=0, l2_reg=0.01)\n\n    # Bottleneck\n    bridge = conv_block(p6, filters=1024, dropout_rate=0, l2_reg=0.01)\n    \n    # Expansive path, decoder\n    u6 = decoder_block(bridge, c6, filters=512, dropout_rate=0, l2_reg=0.01)\n    u5 = decoder_block(u6, c5, filters=256, dropout_rate=0, l2_reg=0.001)\n    u4 = decoder_block(u5, c4, filters=128, dropout_rate=0, l2_reg=0.001)\n    u3 = decoder_block(u4, c3, filters=64, dropout_rate=0, l2_reg=0.0001)\n    u2 = decoder_block(u3, c2, filters=32, dropout_rate=0, l2_reg=0.0001)\n    u1 = decoder_block(u2, c1, filters=16, dropout_rate=0, l2_reg=0.0001)\n\n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(u1)\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n\n    return model","metadata":{"id":"RWFHlE854JPm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unet_model = multi_unet_model(n_classes=n_classes, img_height=IMG_HEIGHT, img_width=IMG_WIDTH, img_channels=3)\n\nprint(f\"Input shape: {unet_model.input_shape}\")\nprint(f\"Output shape: {unet_model.output_shape}\")\nprint(f\"Trainable params: {np.sum([np.prod(v.get_shape()) for v in unet_model.trainable_variables])}\")","metadata":{"id":"-hSO-go4Gg_B","outputId":"aed8d5af-4202-44b2-ddc2-edbb83c40e43","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"id":"YqKAHgHLEpMJ"}},{"cell_type":"code","source":"# Hyperparameters\nEPOCHS = 50\nINITIAL_LR = 1e-4\nSTART_LR = 1e-2\nEND_LR = 1e-4\nDECAY_RATE = 0.96 # Reduce learning rate by 4% every 100 steps\nDECAY_STEPS = len(train_ds) * 400\nCLIP_NORM = 1.0\nCHECKPOINT_PATH = \"training_2/model.h5\"\n\n\n# Model definition\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Rescaling(1./255),  # Normalize input\n    unet_model,\n])\n\n# Learning rate scheduler (Exponential Decay)\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=INITIAL_LR,\n    decay_steps=DECAY_STEPS,\n    decay_rate=DECAY_RATE,\n    staircase=True\n)\n\n# lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n#     starter_learning_rate=START_LR ,\n#     decay_steps=DECAY_STEPS,\n#     end_learning_rate=END_LR,\n#     power=0.5)\n\n# Optimizer with gradient clipping\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=CLIP_NORM)\n\n# Loss function (Categorical Focal Crossentropy)\nloss_fn = tf.keras.losses.CategoricalFocalCrossentropy(alpha=class_weights, gamma=2)\n\n# Model compilation\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n\n# Expected loss calculation (only valid without weight regularization)\nexpected_loss = -np.log(1 / n_classes)\nprint(f\"Expected initial loss: {expected_loss}\")\n\n# Callbacks for training\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        CHECKPOINT_PATH, monitor='val_loss', save_best_only=True, save_freq='epoch'\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1\n    )\n]\n\n# Prepare datasets (remove image names)\ntrain_ds_for_fit = train_ds.map(lambda image, mask, image_name: (image, mask))  # Keep only (image, mask)\nval_ds_for_fit = val_ds.map(lambda image, mask, image_name: (image, mask))\n\n# Model training\nhistory = model.fit(\n    train_ds_for_fit,\n    validation_data=val_ds_for_fit,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Accuracy and Loss plots","metadata":{}},{"cell_type":"code","source":"# Extract training history\nhistory_data = history.history\nepochs = np.arange(1, len(history_data[\"loss\"]) + 1)\n\n# Plot training and validation metrics\nfig, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n# Loss plot\naxes[0].plot(epochs, history_data[\"loss\"], label=\"Training Loss\")\naxes[0].plot(epochs, history_data[\"val_loss\"], label=\"Validation Loss\")\naxes[0].set_title(\"Categorical Focal Cross Entropy\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].grid(True)\naxes[0].legend()\n\n# Accuracy plot\naxes[1].plot(epochs, history_data[\"accuracy\"], label=\"Training Accuracy\")\naxes[1].plot(epochs, history_data[\"val_accuracy\"], label=\"Validation Accuracy\")\naxes[1].set_title(\"Model Accuracy\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Accuracy\")\naxes[1].grid(True)\naxes[1].legend()\n\nplt.show()","metadata":{"id":"ZRLLCKTB-Rn2","outputId":"9fc74e33-36fb-47b4-a1a4-95b64f24f65a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Prepare datasets for evaluation (exclude image names)\ntrain_ds_for_eval = train_ds.map(lambda image, mask, image_name: (image, mask))  # Keep only (image, mask)\nval_ds_for_eval = val_ds.map(lambda image, mask, image_name: (image, mask))\n\ntrain_loss = model.evaluate(train_ds_for_eval)[0]\nval_loss = model.evaluate(val_ds_for_eval)[0]\n\n# Display results\nprint(f\"Final Training Loss: {train_loss:.4f}\")\nprint(f\"Final Validation Loss: {val_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true_list_train = []\ny_pred_list_train = []\ny_true_list_validation = []\ny_pred_list_validation = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test all training data\nfor imageb, maskb, image_nameb in train_ds:\n    for i in range(len(imageb)):\n        image, mask, image_name = imageb[i], maskb[i], image_nameb[i]\n\n        # Print the shape of the image and mask\n        print(f\"Image {i} shape: {image.shape}\")  # Should be (H, W, C) for RGB or (H, W) for grayscale\n        print(f\"Mask {i} shape: {mask.shape}\")  # Should be (H, W) or (H, W, num_classes) if one-hot encoded\n\n        pred = model.predict(image[tf.newaxis, ...])[0] # (height, width, n_classes) , in probabilities\n        pred = tf.math.argmax(pred, axis=-1) # (height, width) , in classes [0, 1, 2, 3, 4,...]\n        pred = tf.cast(tf.one_hot(pred, depth=n_classes), tf.int64) # (height, width, n_classes) , in one hot [0, 0, 0, 0, 1], [0, 1, 0, 0, 0] ...\n\n        print(f\"Pred {i} shape: {pred.shape}\")  # Should be (H, W) or (H, W, num_classes) if one-hot encoded\n\n        y_true_list_train.append(mask.numpy())  # Convert tensor to NumPy (H, W)\n        y_pred_list_train.append(pred.numpy())  # Convert tensor to NumPy (H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test all validation data\nvalidation_image_names = []\n\nfor imageb, maskb, image_nameb in val_ds:\n    for i in range(len(imageb)):\n        image, mask, image_name = imageb[i], maskb[i], image_nameb[i]\n\n        validation_image_names.append(image_name.numpy().decode('utf-8'))\n\n        # Print the shape of the image and mask\n        print(f\"Image {i} shape: {image.shape}\")  # Should be (H, W, C) for RGB or (H, W) for grayscale\n        print(f\"Mask {i} shape: {mask.shape}\")  # Should be (H, W) or (H, W, num_classes) if one-hot encoded\n\n        pred = model.predict(image[tf.newaxis, ...])[0] # (height, width, n_classes) , in probabilities\n        pred = tf.math.argmax(pred, axis=-1) # (height, width) , in classes [0, 1, 2, 3, 4,...]\n        pred = tf.cast(tf.one_hot(pred, depth=n_classes), tf.int64) # (height, width, n_classes) , in one hot [0, 0, 0, 0, 1], [0, 1, 0, 0, 0] ...\n\n        print(f\"Pred {i} shape: {pred.shape}\")  # Should be (H, W) or (H, W, num_classes) if one-hot encoded\n\n        y_true_list_validation.append(mask.numpy())  # Convert tensor to NumPy (H, W)\n        y_pred_list_validation.append(pred.numpy())  # Convert tensor to NumPy (H, W)\n\n        save_predicted_image(image_name,pred)\n\n\n# Write validation image names to txt\nwith open(\"validation_image_names.txt\", \"w\") as file:\n    file.writelines(f\"{item}\\n\" for item in validation_image_names)  # Adds newline after each item\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to NumPy arrays\ny_true_train = np.array(y_true_list_train)  # Shape: (num_images, H, W)\ny_pred_train = np.array(y_pred_list_train)  # Shape: (num_images, H, W)\ny_true_validation = np.array(y_true_list_validation)  # Shape: (num_images, H, W)\ny_pred_validation = np.array(y_pred_list_validation)  # Shape: (num_images, H, W)\nprint(\"y_true_train total validation imgs:\", len(y_true_list_train))  # Should be N * H * W\nprint(\"y_pred_train total validation imgs:\", len(y_pred_list_train))  # Should be N * H * W\nprint(\"y_true_validation total validation imgs:\", len(y_true_list_validation))  # Should be N * H * W\nprint(\"y_pred_validation total validation imgs:\", len(y_pred_list_validation))  # Should be N * H * W\n\nprint(\"y_true_train total pixels:\", y_true_train.size)  # Should be N * H * W\nprint(\"y_pred_train total pixels:\", y_pred_train.size)  # Should be N * H * W\nprint(\"y_pred_validation total pixels:\", y_true_validation.size)  # Should be N * H * W\nprint(\"y_pred_validation total pixels:\", y_pred_validation.size)  # Should be N * H * W","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ny_true_train_flat = y_true_train.flatten()\ny_pred_train_flat = y_pred_train.flatten()\ny_true_validation_flat = y_true_validation.flatten()\ny_pred_validation_flat = y_pred_validation.flatten()\n\n# Compute confusion matrix for train\ncm_train = confusion_matrix(y_true_train_flat, y_pred_train_flat, labels=np.arange(n_classes))\nprint(\"Confusion Matrix Training:\\n\", cm_train)\n\n# Compute confusion matrix for train\ncm_validation = confusion_matrix(y_true_validation_flat, y_pred_validation_flat, labels=np.arange(n_classes))\nprint(\"Confusion Matrix Validation:\\n\", cm_validation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"precision_train, recall_train, f1_score_train = compute_metrics(cm_train)\niou_train, mean_iou_train = compute_iou(cm_train)\n\nprint(f\"Train Precision: {precision_train}\")\nprint(f\"Train Recall: {recall_train}\")\nprint(f\"Train F1 Score: {f1_score_train}\")\n\nprint(f\"Train IoU per class: {iou_train}\")\nprint(f\"Train Mean IoU: {mean_iou_train}\")\n\nprecision_validation, recall_validation, f1_score_validation = compute_metrics(cm_validation)\niou_validation, mean_iou_validation = compute_iou(cm_validation)\n\nprint(f\"Validation Precision: {precision_validation}\")\nprint(f\"Validation Recall: {recall_validation}\")\nprint(f\"Validation F1 Score: {f1_score_validation}\")\n\nprint(f\"Validation IoU per class: {iou_validation}\")\nprint(f\"Validation Mean IoU: {mean_iou_validation}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute accuracy\naccuracy_train = accuracy_score(y_true_train_flat, y_pred_train_flat)\nprint(f\"Train Accuracy: {accuracy_train:.4f}\")\n\naccuracy_validation = accuracy_score(y_true_validation_flat, y_pred_validation_flat)\nprint(f\"Validation Accuracy: {accuracy_validation:.4f}\")\n\n# Another way to Compute precision, recall, F1-score for each class\nprecision_train_2 = precision_score(y_true_train_flat, y_pred_train_flat, average=None)  # Per class\nrecall_train_2 = recall_score(y_true_train_flat, y_pred_train_flat, average=None)  # Per class\n\nprecision_val_2 = precision_score(y_true_validation_flat, y_pred_validation_flat, average=None)  # Per class\nrecall_val_2 = recall_score(y_true_validation_flat, y_pred_validation_flat, average=None)  # Per class\n\n# f1 = f1_score(y_true_flat, y_pred_flat, average=None)  # Per class\nprint(f\"Train Precision method2: {precision_train_2}\")\nprint(f\"Train Recall method2: {recall_train_2}\")\n\nprint(f\"Validation Precision method2: {precision_val_2}\")\nprint(f\"Validation Recall method2: {recall_val_2}\")\n# print(f\"F1-score: {f1}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test on some validation data\n# Use save_predicted_image function to save the predicted images\n\nimageb, maskb, image_nameb = next(iter(val_ds))\n\nfor i in range(len(imageb)):\n    image, mask, image_name = imageb[i], maskb[i], image_nameb[i]\n\n    pred = model.predict(image[tf.newaxis, ...])[0] # (height, width, n_classes) , in probabilities\n    pred = tf.math.argmax(pred, axis=-1) # (height, width) , in classes [0, 1, 2, 3, 4,...]\n    pred = tf.cast(tf.one_hot(pred, depth=n_classes), tf.int64) # (height, width, n_classes) , in one hot [0, 0, 0, 0, 1], [0, 1, 0, 0, 0] ...\n\n    plot_example(image, mask,image_name, pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}